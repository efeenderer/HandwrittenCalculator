{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KaggleDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.12).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/tmdb/tmdb-movie-metadata?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.89M/8.89M [00:01<00:00, 7.33MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\EnderEfe\\.cache\\kagglehub\\datasets\\tmdb\\tmdb-movie-metadata\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"tmdb/tmdb-movie-metadata\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries and Definings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = r\"E:\\\\Python_Projeler\\\\ComputerVisionProjects\\\\FinalProject\\\\dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_dict = {\n",
    "    '0001': 'a', '0002': 'b', '0003': 'c', '0004': 'd', '0005': 'e', '0006': 'f',\n",
    "    '0007': 'h', '0008': 'vertical_line', '0009': 'j', '0010': 'k', '0011': 'vertical_line',\n",
    "    '0012': 'm', '0013': 'n', '0014': 'o', '0015': 'p', '0016': 'q', '0017': 'r',\n",
    "    '0018': 's', '0019': 't', '0020': 'u', '0021': 'v', '0022': 'w', '0023': 'x',\n",
    "    '0024': 'y', '0025': 'z', '0026': '0', '0027': '1', '0028': '2', '0029': '3',\n",
    "    '0030': '4', '0031': '5', '0032': '6', '0033': '7', '0034': '8', '0035': '9',\n",
    "    '0036': 'plus', '0037': 'horizontal_line', '0038': 'slash',\n",
    "    '0039': 'paranthesis_left', '0040': 'paranthesis_right', '0041': 'sqrt', '0042': 'sqrt'}\n",
    "\n",
    "label_dict = {label: 0 for label in set(letter_dict.values())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractSet(path, save_path, name=None):\n",
    "    page = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    kernel = np.ones((4, 4), np.uint8)\n",
    "    blur = cv2.GaussianBlur(page, (3, 3), 1)\n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    dilation = cv2.dilate(thresh, kernel, iterations=2)\n",
    "    dilation_2 = cv2.dilate(thresh, kernel=np.ones((1, 1), np.uint8))  # Optional, may vary per dataset\n",
    "\n",
    "    contours, _ = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for index, contour in enumerate(contours):\n",
    "        mask = np.zeros_like(dilation)\n",
    "        cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "        MaskedLetter = cv2.bitwise_and(dilation_2, dilation_2, mask=mask)\n",
    "\n",
    "        if name is not None:\n",
    "            index = label_dict[name]\n",
    "            label_dict[name] = index + 1\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        max_edge = max(h, w)\n",
    "        blank = np.zeros((max_edge, max_edge), np.uint8)\n",
    "\n",
    "        x1, x2 = int((max_edge - h) / 2), int((max_edge + h) / 2)\n",
    "        y1, y2 = int((max_edge - w) / 2), int((max_edge + w) / 2)\n",
    "\n",
    "        blank[x1:x2, y1:y2] = MaskedLetter[y:y + h, x:x + w]\n",
    "        letter = cv2.resize(blank, (48,48))\n",
    "\n",
    "        cv2.imwrite(save_path + f\"\\\\{index}.jpg\", letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset Image Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dSet_folder in os.listdir(dataset_path):\n",
    "    if len(dSet_folder.split(\".\")) > 1:\n",
    "        continue\n",
    "\n",
    "    dSet_path = os.path.join(dataset_path, dSet_folder)\n",
    "\n",
    "    for index, letter_page in enumerate(os.listdir(dSet_path)):\n",
    "        if letter_page.split(\".\")[-1] != \"jpg\":\n",
    "            continue\n",
    "\n",
    "        letter_page_path = os.path.join(dSet_path, letter_page)\n",
    "        letter_number = letter_page.split(\".\")[0].split(\"-\")[-1]\n",
    "        letter_name = letter_dict[letter_number]\n",
    "\n",
    "        print(f\"{letter_name}        {letter_number}\")\n",
    "\n",
    "        save_path = os.path.join(dSet_path, letter_name)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        ExtractSet(letter_page_path, save_path, letter_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "\n",
    "EMNIST_type = \"balanced\"\n",
    "\n",
    "#This Mapping is for balanced ONLY!! You have to check emnist-TYPE-mapping.txt file to correct the mapping!\n",
    "LabelMapping = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9\",\n",
    "    10: None,\n",
    "    11: None,\n",
    "    12: \"upper_c\",\n",
    "    13: None,\n",
    "    14: None,\n",
    "    15: None,\n",
    "    16: None,\n",
    "    17: None,\n",
    "    18: None,\n",
    "    19: \"upper_j\",\n",
    "    20: \"upper_k\",\n",
    "    21: \"upper_l\",\n",
    "    22: \"upper_m\",\n",
    "    23: None,\n",
    "    24: \"upper_o\",\n",
    "    25: \"upper_p\",\n",
    "    26: None,\n",
    "    27: None,\n",
    "    28: \"upper_s\",\n",
    "    29: None,\n",
    "    30: \"upper_u\",\n",
    "    31: \"upper_v\",\n",
    "    32: \"upper_w\",\n",
    "    33: \"upper_x\",\n",
    "    34: \"upper_y\",\n",
    "    35: \"upper_z\",\n",
    "    36: \"lower_a\",\n",
    "    37: \"lower_b\",\n",
    "    38: \"lower_d\",\n",
    "    39: \"lower_e\",\n",
    "    40: \"lower_f\",\n",
    "    41: \"lower_g\",\n",
    "    42: \"lower_h\",\n",
    "    43: \"lower_n\",\n",
    "    44: \"lower_q\",\n",
    "    45: \"lower_r\",\n",
    "    46: \"lower_t\"\n",
    "}\n",
    "\n",
    "\n",
    "notWanted = ['k', \"j\",\"m\", \"q\", \"z\", \"v\",\"u\"]\n",
    "\n",
    "LabelCounter = {\n",
    "    \"0\": 0,\n",
    "    \"1\": 0,\n",
    "    \"2\": 0,\n",
    "    \"3\": 0,\n",
    "    \"4\": 0,\n",
    "    \"5\": 0,\n",
    "    \"6\": 0,\n",
    "    \"7\": 0,\n",
    "    \"9\": 0,\n",
    "    \"8\": 0,\n",
    "    \"z\": 0,\n",
    "    \"a\": 0,\n",
    "    \"b\": 0,\n",
    "    \"c\": 0,\n",
    "    \"d\": 0,\n",
    "    \"e\": 0,\n",
    "    \"f\": 0,\n",
    "    \"g\": 0,\n",
    "    \"h\": 0,\n",
    "    \"i\": 0,\n",
    "    \"j\": 0,\n",
    "    \"k\": 0,\n",
    "    \"l\": 0,\n",
    "    \"m\": 0,\n",
    "    \"n\": 0,\n",
    "    \"o\": 0,\n",
    "    \"p\": 0,\n",
    "    \"q\": 0,\n",
    "    \"r\": 0,\n",
    "    \"s\": 0,\n",
    "    \"t\": 0,\n",
    "    \"u\": 0,\n",
    "    \"v\": 0,\n",
    "    \"w\": 0,\n",
    "    \"x\": 0,\n",
    "    \"y\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadIdxImage(file):\n",
    "    with gzip.open(file,\"rb\") as F:\n",
    "        \n",
    "        magic_number = int.from_bytes(F.read(4), 'big') \n",
    "        num_items = int.from_bytes(F.read(4), 'big')  \n",
    "        num_rows = int.from_bytes(F.read(4), 'big') \n",
    "        num_cols = int.from_bytes(F.read(4), 'big')  \n",
    "\n",
    "        data = np.frombuffer(F.read(),dtype = np.uint8)\n",
    "        data = data.reshape(num_items,num_rows,num_cols)\n",
    "\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def ReadIdxLabel(file):\n",
    "    with gzip.open(file,\"rb\") as F:\n",
    "\n",
    "        F.read(8)\n",
    "\n",
    "    \n",
    "        return F.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ../../dataset/EMNIST/balanced/train/0 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/1 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/2 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/3 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/4 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/5 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/6 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/7 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/8 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/9 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/c is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/j is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/k is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/l is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/m is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/o is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/p is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/s is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/u is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/v is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/w is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/x is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/y is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/z is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/a is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/b is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/d is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/e is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/f is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/g is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/h is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/n is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/q is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/r is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/train/t is created or already exists\n",
      "EMNIST Dataset Extraction is completed.\n",
      "{'0': 2400, '1': 2400, '2': 2400, '3': 2400, '4': 2400, '5': 2400, '6': 2400, '7': 2400, '9': 2400, '8': 2400, 'z': 2400, 'a': 2400, 'b': 2400, 'c': 2400, 'd': 2400, 'e': 2400, 'f': 2400, 'g': 2400, 'h': 2400, 'i': 0, 'j': 2400, 'k': 2400, 'l': 2400, 'm': 2400, 'n': 2400, 'o': 2400, 'p': 2400, 'q': 2400, 'r': 2400, 's': 2400, 't': 2400, 'u': 2400, 'v': 2400, 'w': 2400, 'x': 2400, 'y': 2400}\n",
      "Folder ../../dataset/EMNIST/balanced/test/0 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/1 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/2 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/3 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/4 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/5 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/6 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/7 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/8 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/9 is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/c is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/j is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/k is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/l is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/m is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/o is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/p is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/s is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/u is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/v is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/w is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/x is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/y is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/z is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/a is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/b is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/d is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/e is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/f is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/g is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/h is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/n is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/q is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/r is created or already exists\n",
      "Folder ../../dataset/EMNIST/balanced/test/t is created or already exists\n",
      "EMNIST Dataset Extraction is completed.\n",
      "{'0': 2800, '1': 2800, '2': 2800, '3': 2800, '4': 2800, '5': 2800, '6': 2800, '7': 2800, '9': 2800, '8': 2800, 'z': 2800, 'a': 2800, 'b': 2800, 'c': 2800, 'd': 2800, 'e': 2800, 'f': 2800, 'g': 2800, 'h': 2800, 'i': 0, 'j': 2800, 'k': 2800, 'l': 2800, 'm': 2800, 'n': 2800, 'o': 2800, 'p': 2800, 'q': 2800, 'r': 2800, 's': 2800, 't': 2800, 'u': 2800, 'v': 2800, 'w': 2800, 'x': 2800, 'y': 2800}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for train_or_test in [\"train\", \"test\"]:\n",
    "    \n",
    "    images_path = r\"EMNIST/\"+EMNIST_type+r\"/emnist-\"+EMNIST_type+r\"-\"+train_or_test+\"-images-idx3-ubyte.gz\"\n",
    "    labels_path = r\"EMNIST/\"+EMNIST_type+r\"/emnist-\"+EMNIST_type+r\"-\"+train_or_test+\"-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    save_path = r\"../../dataset/EMNIST/\"+EMNIST_type + r\"/\" +train_or_test\n",
    "\n",
    "\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    images = ReadIdxImage(images_path)\n",
    "\n",
    "    labels = ReadIdxLabel(labels_path)\n",
    "\n",
    "    for label in LabelMapping:\n",
    "        \n",
    "        char = LabelMapping[label]\n",
    "        if char is None:    continue\n",
    "        char = char[-1]\n",
    "        \n",
    "        path = save_path + r\"/\" + char\n",
    "        os.makedirs(path, exist_ok=True )\n",
    "        print(f\"Folder {path} is created or already exists\")\n",
    "\n",
    "\n",
    "\n",
    "    error_text = \"\"\n",
    "\n",
    "    for i,(image, label) in enumerate(zip(images,labels)):\n",
    "        if LabelMapping[label] is None:\n",
    "            continue\n",
    "\n",
    "        char = LabelMapping[label][-1]\n",
    "        \n",
    "        if char in notWanted:   \n",
    "            LabelCounter[char] += 1\n",
    "            continue\n",
    "\n",
    "        char_index = LabelCounter[char]  \n",
    "\n",
    "        try:\n",
    "            save = save_path + r\"/\" + char +r\"/\"\n",
    "\n",
    "            image = cv2.resize(image,(32,32), interpolation=cv2.INTER_NEAREST)\n",
    "            cv2.imwrite(save + f\"{char_index}.png\",image.T)\n",
    "            LabelCounter[char] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            error_text+= f\"There was an error on {char} - {EMNIST_type}  -    Error: {e}\\n\\n\"\n",
    "\n",
    "    with open(\"/datasetC_error_log.txt\", \"a+\") as F:\n",
    "        F.write(error_text)\n",
    "\n",
    "    print(f\"EMNIST Dataset Extraction is completed.\")\n",
    "    print(LabelCounter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators 2800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " + is DONE!\n",
      " - is DONE!\n",
      " sqrt is DONE!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path = r\"../../dataset/Operators/\"\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "\n",
    "    if len(os.listdir(path + folder)) <= 2800:     continue\n",
    "\n",
    "    for image in os.listdir(path + folder)[2800:]:\n",
    "        os.remove( os.path.join(path + folder, image) )\n",
    "    print(f\" {folder} is DONE!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ( is done\n",
      "Folder ) is done\n",
      "Folder + is done\n",
      "Folder - is done\n",
      "Folder slash is done\n",
      "Folder sqrt is done\n",
      "Folder [ is done\n",
      "Folder ] is done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path = r\"../../dataset/Operators/\"\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "\n",
    "    if folder == \"1\":   continue\n",
    "\n",
    "    \n",
    "    for i, image in enumerate( os.listdir(path + folder) ):\n",
    "        image_path = os.path.join(path + folder, image)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        blur = cv2.GaussianBlur(img, (3, 3), 1)\n",
    "        _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        dilation = cv2.dilate(thresh, kernel=np.ones((2,2), np.uint8))\n",
    "\n",
    "        cv2.imwrite(image_path, dilation)\n",
    "\n",
    "    print(f\"Folder {folder} is done\")\n",
    "\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ( is done\n",
      "Folder ) is done\n",
      "Folder + is done\n",
      "Folder - is done\n",
      "Folder 1 is done\n",
      "Folder slash is done\n",
      "Folder sqrt is done\n",
      "Folder [ is done\n",
      "Folder ] is done\n"
     ]
    }
   ],
   "source": [
    "path = r\"../../dataset/Operators/\"\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "    \n",
    "    for i, image in enumerate( os.listdir(path + folder) ):\n",
    "        image_path = os.path.join(path + folder, image)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = cv2.resize(img, (32,32), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        cv2.imwrite(image_path, img)\n",
    "\n",
    "    print(f\"Folder {folder} is done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ( is blurred\n",
      "Folder ) is blurred\n",
      "Folder + is blurred\n",
      "Folder - is blurred\n",
      "Folder 1 is blurred\n",
      "Folder slash is blurred\n",
      "Folder sqrt is blurred\n",
      "Folder [ is blurred\n",
      "Folder ] is blurred\n"
     ]
    }
   ],
   "source": [
    "path = r\"../../dataset/Operators/\"\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "    \n",
    "    for i, image in enumerate( os.listdir(path + folder) ):\n",
    "\n",
    "        image_path = os.path.join(path + folder, image)\n",
    "\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = cv2.GaussianBlur(img, (3,3), 1)\n",
    "\n",
    "        cv2.imwrite(image_path, img)\n",
    "\n",
    "    print(f\"Folder {folder} is blurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMNIST Renaming\n",
    "\n",
    "Might need this later. Right now, it's not much useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMNIST_balanced_path = r\"../../dataset/EMNIST/balanced/\"\n",
    "train = EMNIST_balanced_path + r\"/train\"\n",
    "test = EMNIST_balanced_path + r\"/test\"\n",
    "\n",
    "for char in os.listdir(train):\n",
    "    char_path = train + r\"/\" + char\n",
    "    for i, image_name in enumerate(os.listdir(char_path)):\n",
    "\n",
    "        try:\n",
    "            path = os.path.join(char_path, image_name)\n",
    "\n",
    "            os.rename(path, os.path.join(char_path, f\"EMNIST_{image_name}\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            text = f\"Error at train rename: {e}\"\n",
    "\n",
    "            with open(\"datasetC_error_log.txt\", \"a+\") as F:\n",
    "                F.write(text)\n",
    "\n",
    "\n",
    "for char in os.listdir(test):\n",
    "    char_path = test + r\"/\" + char\n",
    "    \n",
    "    for i, image_name in enumerate(os.listdir(char_path)):\n",
    "\n",
    "        try:\n",
    "            path = os.path.join(char_path, image_name)\n",
    "\n",
    "            os.rename(path, os.path.join(char_path, f\"EMNIST_{image_name}\"))\n",
    "            \n",
    "        except Exception as e:\n",
    "            text = f\"Error at test rename: {e}\"\n",
    "\n",
    "            with open(\"datasetC_error_log.txt\", \"a+\") as F:\n",
    "                F.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Sets Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "EMNIST_balanced_path = r\"../../dataset/EMNIST/balanced/\"\n",
    "Operator_path = r\"../../dataset/Operators/\"\n",
    "Custom_set_1 = r\"../../dataset/berna\"\n",
    "Custom_set_2 = r\"../../dataset/ender\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMNIST Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset/mainDataset\\train\\0\n",
      "train, char: 0 is Finished!\n",
      "../../dataset/mainDataset\\train\\1\n",
      "train, char: 1 is Finished!\n",
      "../../dataset/mainDataset\\train\\2\n",
      "train, char: 2 is Finished!\n",
      "../../dataset/mainDataset\\train\\3\n",
      "train, char: 3 is Finished!\n",
      "../../dataset/mainDataset\\train\\4\n",
      "train, char: 4 is Finished!\n",
      "../../dataset/mainDataset\\train\\5\n",
      "train, char: 5 is Finished!\n",
      "../../dataset/mainDataset\\train\\6\n",
      "train, char: 6 is Finished!\n",
      "../../dataset/mainDataset\\train\\7\n",
      "train, char: 7 is Finished!\n",
      "../../dataset/mainDataset\\train\\8\n",
      "train, char: 8 is Finished!\n",
      "../../dataset/mainDataset\\train\\9\n",
      "train, char: 9 is Finished!\n",
      "../../dataset/mainDataset\\train\\a\n",
      "train, char: a is Finished!\n",
      "../../dataset/mainDataset\\train\\b\n",
      "train, char: b is Finished!\n",
      "../../dataset/mainDataset\\train\\c\n",
      "train, char: c is Finished!\n",
      "../../dataset/mainDataset\\train\\d\n",
      "train, char: d is Finished!\n",
      "../../dataset/mainDataset\\train\\e\n",
      "train, char: e is Finished!\n",
      "../../dataset/mainDataset\\train\\f\n",
      "train, char: f is Finished!\n",
      "../../dataset/mainDataset\\train\\g\n",
      "train, char: g is Finished!\n",
      "../../dataset/mainDataset\\train\\h\n",
      "train, char: h is Finished!\n",
      "../../dataset/mainDataset\\train\\j\n",
      "train, char: j is Finished!\n",
      "../../dataset/mainDataset\\train\\k\n",
      "train, char: k is Finished!\n",
      "../../dataset/mainDataset\\train\\l\n",
      "train, char: l is Finished!\n",
      "../../dataset/mainDataset\\train\\m\n",
      "train, char: m is Finished!\n",
      "../../dataset/mainDataset\\train\\n\n",
      "train, char: n is Finished!\n",
      "../../dataset/mainDataset\\train\\o\n",
      "train, char: o is Finished!\n",
      "../../dataset/mainDataset\\train\\p\n",
      "train, char: p is Finished!\n",
      "../../dataset/mainDataset\\train\\q\n",
      "train, char: q is Finished!\n",
      "../../dataset/mainDataset\\train\\r\n",
      "train, char: r is Finished!\n",
      "../../dataset/mainDataset\\train\\s\n",
      "train, char: s is Finished!\n",
      "../../dataset/mainDataset\\train\\t\n",
      "train, char: t is Finished!\n",
      "../../dataset/mainDataset\\train\\u\n",
      "train, char: u is Finished!\n",
      "../../dataset/mainDataset\\train\\v\n",
      "train, char: v is Finished!\n",
      "../../dataset/mainDataset\\train\\w\n",
      "train, char: w is Finished!\n",
      "../../dataset/mainDataset\\train\\x\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m         src = os.path.join(char_folder_path , image_file)\n\u001b[32m     26\u001b[39m         drc = os.path.join(main_set_char_folder, image_file)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy2\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_or_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, char: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is Finished!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programlar\\Python\\python3117\\Lib\\shutil.py:436\u001b[39m, in \u001b[36mcopy2\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(dst):\n\u001b[32m    435\u001b[39m     dst = os.path.join(dst, os.path.basename(src))\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m copystat(src, dst, follow_symlinks=follow_symlinks)\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programlar\\Python\\python3117\\Lib\\shutil.py:256\u001b[39m, in \u001b[36mcopyfile\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    254\u001b[39m     os.symlink(os.readlink(src), dst)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[32m    257\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    258\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[32m    259\u001b[39m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "main_set =r\"../../dataset/mainDataset\"\n",
    "\n",
    "os.makedirs(main_set, exist_ok=True)\n",
    "os.makedirs(main_set+r\"/train\", exist_ok=True)\n",
    "os.makedirs(main_set+r\"/test\", exist_ok=True)\n",
    "\n",
    "for train_or_test in [\"train\",\"test\"]:\n",
    "\n",
    "    main_set_train_test_path = os.path.join(main_set,train_or_test)\n",
    "\n",
    "    EMNIST_balanced_path_path = os.path.join(EMNIST_balanced_path, train_or_test)\n",
    "\n",
    "    for char_folder in os.listdir(EMNIST_balanced_path_path):\n",
    "\n",
    "        try:\n",
    "            char_folder_path = os.path.join(EMNIST_balanced_path_path,char_folder)\n",
    "\n",
    "            main_set_char_folder = os.path.join(main_set_train_test_path, char_folder)\n",
    "            print(main_set_char_folder)\n",
    "\n",
    "            os.makedirs( main_set_char_folder , exist_ok=True )\n",
    "\n",
    "            for image_file in os.listdir(char_folder_path):\n",
    "\n",
    "                src = os.path.join(char_folder_path , image_file)\n",
    "                drc = os.path.join(main_set_char_folder, image_file)\n",
    "                shutil.copy2(src,drc)\n",
    "\n",
    "            print(f\"{train_or_test}, char: {char_folder} is Finished!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            text = f\"Error at test rename: {e}\"\n",
    "\n",
    "            with open(\"datasetC_error_log.txt\", \"a+\") as F:\n",
    "                F.write(text)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operators Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer is done for (!\n",
      "Transfer is done for )!\n",
      "Transfer is done for +!\n",
      "Transfer is done for -!\n",
      "Transfer is done for 1!\n",
      "Transfer is done for slash!\n",
      "Transfer is done for sqrt!\n",
      "Transfer is done for [!\n",
      "Transfer is done for ]!\n"
     ]
    }
   ],
   "source": [
    "operator_path = r\"../../dataset/Operators/\"\n",
    "save_path = r\"../../dataset/Operators_splitted/\"\n",
    "train_ratio = 0.8\n",
    "\n",
    "\n",
    "train_path = os.path.join(save_path, \"train\")\n",
    "test_path = os.path.join(save_path, \"test\")\n",
    "\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for operator_name in os.listdir(operator_path):\n",
    "    operator_dir = os.path.join(operator_path, operator_name)\n",
    "    if not os.path.isdir(operator_dir):\n",
    "        continue  \n",
    "\n",
    "\n",
    "    all_images = [f for f in os.listdir(operator_dir) if os.path.isfile(os.path.join(operator_dir, f))]\n",
    "    random.shuffle(all_images)\n",
    "\n",
    "    split_idx = int(len(all_images) * train_ratio)\n",
    "    train_images = all_images[:split_idx]\n",
    "    test_images = all_images[split_idx:]\n",
    "\n",
    "\n",
    "    train_operator_dir = os.path.join(train_path, operator_name)\n",
    "    test_operator_dir = os.path.join(test_path, operator_name)\n",
    "    os.makedirs(train_operator_dir, exist_ok=True)\n",
    "    os.makedirs(test_operator_dir, exist_ok=True)\n",
    "    try:\n",
    "\n",
    "        for img_name in train_images:\n",
    "            src = os.path.join(operator_dir, img_name)\n",
    "            dst = os.path.join(train_operator_dir, img_name)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "        for img_name in test_images:\n",
    "            src = os.path.join(operator_dir, img_name)\n",
    "            dst = os.path.join(test_operator_dir, img_name)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "        print(f\"Transfer is done for {operator_name}!\")\n",
    "    except Exception as e:\n",
    "        text = f\"Error at test rename: {e}\"\n",
    "\n",
    "        with open(\"datasetC_error_log.txt\", \"a+\") as F:\n",
    "            F.write(text)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
